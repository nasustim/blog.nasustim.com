{"componentChunkName":"component---src-page-components-tag-articles-tsx","path":"/tag/techmemo/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"frontmatter":{"date":"June 07, 2025","slug":"introduce-ollama-into-macos","title":"Ollama 導入と VSCode との連携","draft":false,"tags":["TechMemo","LLM"]},"html":"<p>IT 業界では常に話題の中心にいる LLM。2025年の現在、僕も職場では LLM ツール<sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup>にどっぷり浸かる毎日を過ごしています。しかし、高性能なモデルや API 経由での利用をしたい場合は追加課金が発生することが多いです。</p>\n<p>ローカルマシンで LLM をホストできれば課金の必要はありません。また、導入プロセスを通じて LLM への理解をより深めたいと考えました。</p>\n<p>そこでこの記事では、ローカルマシンへの LLM の導入から VSCode 上でコード編集機能を利用するまでにやったことを記録として残します。</p>\n<p>動作を確認した環境は以下の通りです。</p>\n<ul>\n<li>PC\n<ul>\n<li>Macbook Air M2 (2022年)</li>\n<li>RAM 16GB</li>\n</ul>\n</li>\n<li>OS\n<ul>\n<li>macOS Sequoia v15.3 (24D60)</li>\n</ul>\n</li>\n</ul>\n<h2>調査</h2>\n<p>Gemini に頼りました。</p>\n<p><img src=\"/introduce-ollama-into-macos_01.png\" alt=\"Gemini の回答\"></p>\n<p>会話リンク: <a href=\"https://g.co/gemini/share/779a99352554\">https://g.co/gemini/share/779a99352554</a></p>\n<p>指示された手順はざっくり以下の通りです。</p>\n<ol>\n<li><a href=\"http://ollama.com/\">Ollama</a> をインストールして LLM を起動\n<ul>\n<li>HTTP で LLM とやり取りできるサーバがバンドルされているらしい</li>\n</ul>\n</li>\n<li>VSCode に <a href=\"https://www.continue.dev/\">Continue</a> の拡張をインストールして ①の LLM を指定する</li>\n<li>LLM を使ってコードを書ける！</li>\n</ol>\n<h2>導入の流れ</h2>\n<h3>1. Ollama 導入</h3>\n<p>似たソフトウェアとして <a href=\"https://lmstudio.ai/\">LM Studio</a> や、複数の LLM を一つのホストに集約することに主眼を置いたルーター系（勝手に命名しました）のプロジェクト<sup id=\"fnref-2\"><a href=\"#fn-2\" class=\"footnote-ref\">2</a></sup>がありますが、Ollama はそれらと比較したときに以下のようなアドバンテージがあると理解しました。</p>\n<ul>\n<li>高いポータビリティ\n<ul>\n<li>コマンドラインのみで導入可能</li>\n<li>システムプロンプト等のカスタマイズ内容も Modelfile という Dockerfile ライクな形式のファイルで表現可能<sup id=\"fnref-3\"><a href=\"#fn-3\" class=\"footnote-ref\">3</a></sup></li>\n</ul>\n</li>\n<li>VSCode への導入サポートの手厚さ\n<ul>\n<li>前述の Continue を使って楽チン</li>\n</ul>\n</li>\n</ul>\n<p>今回は Gemini の指示通り Ollama で進めます。</p>\n<pre><code class=\"language-bash\"># ollama を Homebrew でインストール\n$ brew install ollama\n\n# ollama のサーバを起動\n# この後は起動したまま別のターミナルから操作するか、コマンド末尾に `&#x26;` をつけてバックグラウンドジョブとして動かすかはお好みで\n$ ollama serve\n</code></pre>\n<p>ollama では様々なモデルを利用できます。（<a href=\"https://ollama.com/search\">モデル一覧</a>）\n最初は軽量かつ突飛な動作をしないであろうモデルを試したいと考え、Google が開発した Gemma 3 の 1B パラメータモデル<sup id=\"fnref-4\"><a href=\"#fn-4\" class=\"footnote-ref\">4</a></sup>を選択しました。</p>\n<pre><code class=\"language-bash\">$ ollama pull gemma3:1b\n</code></pre>\n<p>コマンドライン経由でのチャットも行えるようです。</p>\n<p><img src=\"/introduce-ollama-into-macos_02.png\" alt=\"Gemma 3 の回答\"></p>\n<p>シャ、シャベッタ！</p>\n<p>ローカル LLM 導入のファーストステップはクリアです ✅</p>\n<h3>2. VSCode に Continue をインストール &#x26; 設定</h3>\n<table>\n<thead>\n<tr>\n<th>説明</th>\n<th>スクリーンショット</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>インストール後初期画面</td>\n<td><img src=\"/introduce-ollama-into-macos_03.png\" alt=\"Continue\"></td>\n</tr>\n<tr>\n<td>Get started を開くとこの画面が開くので Ollama を選択する</td>\n<td><img src=\"/introduce-ollama-into-macos_04.png\" alt=\"Continue\"></td>\n</tr>\n</tbody>\n</table>\n<h3>3. Ollama(Gemma 3)にコードを編集させる</h3>\n<p>手順2の設定を終えたら、モデル一覧から Gemma 3 が選択できるようになっています。</p>\n<p><img src=\"/introduce-ollama-into-macos_05.png\" alt=\"Continue\"></p>\n<p>チャットは問題なさそうです。</p>\n<p><img src=\"/introduce-ollama-into-macos_06.png\" alt=\"Continue\"></p>\n<p>コードを選択して Add Highlighted Code to Context でチャット画面に指示のコンテクストとして追加できます。Copilot / Cursor と同じインターフェースですね。</p>\n<p><img src=\"/introduce-ollama-into-macos_07.png\" alt=\"Continue\"></p>\n<p>うーん、これじゃない...業務で使う Claude 3.7 Sonnet 辺りのモデルよりも明らかな知識/文脈理解の不足を感じます。より賢くてコーディングに特化したモデルを使う必要がありそうです。</p>\n<p>ということでコーディング特化モデルの中で　pull 数が多くサイズも大きすぎない <code>qwen2.5-coder:7b</code> を試しました</p>\n<p><a href=\"https://ollama.com/library/qwen2.5-coder\">https://ollama.com/library/qwen2.5-coder</a></p>\n<p><img src=\"/introduce-ollama-into-macos_08.png\" alt=\"Qwen 2.5 coder\"></p>\n<p>こちらは期待通りの出力が得られました 🙌</p>\n<h2>まとめ</h2>\n<p>シンプルな手順で LLM 導入 + VSCode 連携まで構築できました。</p>\n<p>また、シンプルなタスクであれば精度高く実行できるモデルがローカルでホストできることも確認できました。</p>\n<p>個人用途ならローカルで十分かもしれません。夢が広がりますね</p>\n<hr>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-1\">記事執筆時点では、調べ物は ChatGPT（プライベートでは Gemini と Grok を使い分け）、一つの文書を元にしたい場合は NotebookLM、コーディング時は GitHub Copilot と Devin をよく利用しています<a href=\"#fnref-1\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-2\">サクッと調べると <a href=\"https://github.com/kcolemangt/llm-router\">LLM Router</a>, <a href=\"https://github.com/lm-sys/RouteLLM\">RouteLLM</a> 辺りの情報が多いように感じました<a href=\"#fnref-2\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-3\"><a href=\"https://ollama.readthedocs.io/en/modelfile/\">https://ollama.readthedocs.io/en/modelfile/</a><a href=\"#fnref-3\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-4\"><a href=\"https://ollama.com/library/gemma3\">https://ollama.com/library/gemma3</a> ; 1b=10億パラメータの機械学習モデルをラップトップでサクッと動かせることにビビりました<a href=\"#fnref-4\" class=\"footnote-backref\">↩</a></li>\n</ol>\n</div>"}}]}},"pageContext":{"tag":"TechMemo","tagSlug":"techmemo","limit":10,"skip":0,"pagesCount":1,"currentPageIndex":0}},"staticQueryHashes":["1865044719"],"slicesMap":{}}