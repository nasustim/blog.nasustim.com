{"componentChunkName":"component---src-page-components-entry-article-tsx","path":"/entry/introduce-ollama-into-macos/","result":{"data":{"markdownRemark":{"rawMarkdownBody":"\nIT 業界では常に話題の中心に LLM がいる2025年。僕も職場では LLM ツール[^1]にどっぷり浸かる毎日を過ごしています。しかし高性能なモデルや API 経由での利用をしたい場合は追加課金が発生する多いです。\n\nそんな時、「自身のローカルマシンで LLM をホストすれば無料でできることが増えるかも」と考えました。そこでこの記事ではローカルマシンへの LLM の導入から VSCode 上でコード編集機能を利用するまでにやったことを記録として残します。\n\nちなみに私のローカルマシンは以下のスペックです\n\n| K | V |\n| --- | --- |\n| コンピュータ | Macbook Air M2 (2022年); RAM 16GB |\n| OS | macOS Sequoia v15.3 (24D60) |\n\n## 調査\n\nGemini に頼りました\n\n![Gemini の回答](/introduce-ollama-into-macos_01.png)\n\n会話リンク: https://g.co/gemini/share/779a99352554\n\n指示された手順はざっくり以下の通り\n\n1. [Ollama](http://ollama.com/) をインストールして LLM を起動\n  - HTTP で　LLM とやり取りできるサーバがバンドルされているらしい\n1. VSCode に [Continue](https://www.continue.dev/) の拡張をインストールして ①の LLM を指定する\n1. LLM を使ってコードを書ける！\n\n## 導入の流れ\n\n### 1. Ollama 導入\n\n似たソフトウェアとして [LM Studio](https://lmstudio.ai/) や、複数の LLM を一つのホストに集約することに主眼を置いたルーター系（勝手に命名しました）のプロジェクト[^2]がありますが、Ollama はそれらと比較したときに以下のようなアドバンテージがあると理解しました。\n\n- 高いポータビリティ\n  - コマンドラインのみで導入可能\n  - システムプロンプト等カスタマイズの内容も Modelfile という Dockerfile ライクな形式のファイルで表現可能[^3]\n- VSCode への導入サポートの手厚さ\n  - 前述の Continue を使って楽ちんでした\n\n今回は Gemini の指示通り Ollama で進めます\n\n```bash\n# ollama を Homebrew でインストール\n$ brew install ollama\n\n# ollama のサーバを起動\n# この後は起動したまま別のターミナルから操作するか、コマンド末尾に `&` をつけてバックグラウンドジョブとして動かすかはお好みで\n$ ollama serve\n```\n\nollama では様々なモデルを利用できます。（[モデル一覧](https://ollama.com/search)）\n最初は軽量かつ突飛な動作をしないであろうモデルを試したいと考え、Google が開発した　Gemma 3 の 1B パラメータモデル[^4]を選択しました。\n\n\n```bash\n$ ollama pull gemma3:1b\n```\n\nコマンドライン経由でのチャットも行えるようです。\n\n\n![Gemma 3 の回答](/introduce-ollama-into-macos_02.png)\n\nシャ、シャベッタ！\n\nローカル LLM 導入のファーストステップはクリアです ✅ \n\n### 2. VSCode に　Continue をインストール & 設定\n\n| 説明 | スクリーンショット |\n| --- | --- |\n|  インストール後初期画面 | ![Continue](/introduce-ollama-into-macos_03.png) |\n| Get started を開くとこの画面が開くので Ollama を選択する | ![Continue](/introduce-ollama-into-macos_04.png) |\n\n### 3. Ollama(Gemma 3)にコードを編集させる\n\n2 の設定を終えたらモデル一覧から Gemma 3 が選択できるようになっています。\n\nチャットは問題なさそう\n\n![Continue](/introduce-ollama-into-macos_05.png)\n\n\nコードを選択して Add Highlighted Code to Context でチャット画面に指示のコンテクストとして追加できます。Copilot / Cursor と同じインターフェースですね。\n\n![Continue](/introduce-ollama-into-macos_06.png)\n\n\nうーん、これじゃない...明らかに業務で使う Claude Sonnet-3.7 辺りのモデルよりも知識/文脈理解の不足を感じます\n\n![Continue](/introduce-ollama-into-macos_07.png)\n\nとはいえコード編集をさせるまで成功しました。高性能モデルを使えばより精度の高い出力が得られることでしょう\n\n## まとめ\n\nかなりシンプルな手順で LLM 導入 + VSCode 連携まで作成できました。\n\n今後より少ないリソースでより賢く振る舞うモデルはすぐ生まれるでしょう。 Copilot / Cursor に課金しなくても Vibe Coding ができる未来は近いかもしれません。\n\n----\n\n[^1]: 記事執筆時点では、調べ物は ChatGPT（プライベートでは Gemini と Grok を使い分け）、一つの文書を元にしたい場合は NotebookLM 、コーディング時は GitHub Copilot と Devin をよく利用してます\n[^2]: サクッと調べると　[LLM Router](https://github.com/kcolemangt/llm-router), [RouteLLM](https://github.com/lm-sys/RouteLLM) 辺りの情報が多いように感じました\n[^3]: https://ollama.readthedocs.io/en/modelfile/\n[^4]: https://ollama.com/library/gemma3 ; 1b=10億パラメータの機械学習モデルをラップトップで動かせるなんて...コンピュータの高性能化は凄まじいですね \n","frontmatter":{"date":"June 07, 2025","slug":"introduce-ollama-into-macos","title":"macOS に Ollama を導入して VSCode と連携させるまで","draft":false}},"site":{"siteMetadata":{"title":"NASUSTIM","description":"a weblog by @nasustim (Mitsuhiro Hibino)","siteUrl":"https://blog.nasustim.com"}}},"pageContext":{"id":"943a6afd-f230-506f-8d5b-460c6a07cc31"}},"staticQueryHashes":["1865044719"],"slicesMap":{}}