<!DOCTYPE html><html lang="ja"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="generator" content="Gatsby 5.14.4"/><meta name="description" content="
IT 業界では常に話題の中心にいる LLM。2025年の現在、僕も職場では LLM ツール[^1]にどっぷり浸かる毎日を過ごしています。しかし、高性能なモデルや API 経由での利用をしたい場合は追加課金が発生することが多いです。

ローカルマシンで LLM をホストできれば課金の必要はありません。また、導入プロセス" data-gatsby-head="true"/><meta property="og:title" content="Ollama 導入と VSCode との連携 | NASUSTIM" data-gatsby-head="true"/><meta property="og:description" content="
IT 業界では常に話題の中心にいる LLM。2025年の現在、僕も職場では LLM ツール[^1]にどっぷり浸かる毎日を過ごしています。しかし、高性能なモデルや API 経由での利用をしたい場合は追加課金が発生することが多いです。

ローカルマシンで LLM をホストできれば課金の必要はありません。また、導入プロセス" data-gatsby-head="true"/><meta property="og:image" content="static/favicon-48x48.png" data-gatsby-head="true"/><meta property="og:url" content="https://blog.nasustim.com/entry/introduce-ollama-into-macos" data-gatsby-head="true"/><meta property="og:type" content="article" data-gatsby-head="true"/><meta property="twitter:card" content="summary" data-gatsby-head="true"/><meta property="twitter:url" content="https://blog.nasustim.com/entry/introduce-ollama-into-macos" data-gatsby-head="true"/><meta property="twitter:title" content="Ollama 導入と VSCode との連携 | NASUSTIM" data-gatsby-head="true"/><meta property="twitter:image" content="static/favicon-48x48.png" data-gatsby-head="true"/><meta property="twitter:description" content="
IT 業界では常に話題の中心にいる LLM。2025年の現在、僕も職場では LLM ツール[^1]にどっぷり浸かる毎日を過ごしています。しかし、高性能なモデルや API 経由での利用をしたい場合は追加課金が発生することが多いです。

ローカルマシンで LLM をホストできれば課金の必要はありません。また、導入プロセス" data-gatsby-head="true"/><meta property="twitter:creator" content="@nasustim" data-gatsby-head="true"/><style data-href="/styles.e8ccf58d1de51d48b060.css" data-identity="gatsby-global-css">.uxe4kj0{--uxe4kj1:#494757;--uxe4kj2:#d94b95;--uxe4kj3:#dddddc}._2danq60{width:100%}._2danq61{margin:16px 0}._2danq62{font-size:24px;font-weight:400}._2danq63>*{margin-top:24px}._2danq63 p img{max-width:100%}._2danq63 h1{font-size:28px;font-weight:450}._2danq63 h2{font-size:25px;font-weight:400}._2danq63 h3{font-size:22px;font-weight:350}._2danq63 h4{font-size:19px;font-weight:300}._2danq63 h5{font-size:16px;font-weight:250}._2danq63 ul{list-style-type:disc;margin-left:24px}._2danq63 ol{list-style-type:decimal;margin-left:24px}._2danq63 li{padding-top:8px}._2danq63 a{color:var(--uxe4kj1)}._2danq63 a:hover{color:var(--uxe4kj2)}._2danq63 p code,._2danq63 ul code{margin:0 4px;padding:1px 4px}._2danq63 p code,._2danq63 ul code,._2danq63>code,._2danq63>pre code{background-color:#efefef;border:1px solid #b6b6b6;border-radius:4px;font-family:monospace}._2danq63>code,._2danq63>pre code{display:block;overflow-wrap:normal;overflow-x:scroll;padding:4px 12px;width:100%}._2danq63>table{border-collapse:collapse;overflow-x:scroll}._2danq63>table>thead{background-color:#b6b6b6}._2danq63>table>tbody tr:nth-child(2n){background-color:#efefef}._2danq63>table td,._2danq63>table tr{padding:10px}._2danq63 sup>a{font-weight:700}._2danq63 sup>a:before{content:"["}._2danq63 sup>a:after{content:"]"}._2danq63 .footnotes ol{padding-left:16px}._2danq63 .footnotes ol li{list-style:auto}*,:after,:before{box-sizing:border-box;margin:0;padding:0}:where([hidden]:not([hidden=until-found])){display:none!important}:where(html){-webkit-text-size-adjust:none;interpolate-size:allow-keywords;color-scheme:dark light;line-height:1.5;scrollbar-gutter:stable;tab-size:2}:where(html:has(dialog:modal[open])){overflow:clip}@media (prefers-reduced-motion:no-preference){:where(html:focus-within){scroll-behavior:smooth}}:where(body){-webkit-font-smoothing:antialiased;font-family:system-ui,sans-serif;line-height:inherit}:where(button){all:unset}:where(input,button,textarea,select){font-feature-settings:inherit;color:inherit;font:inherit;font-variation-settings:inherit;letter-spacing:inherit;word-spacing:inherit}:where(textarea){resize:vertical;resize:block}:where(button,label,select,summary,[role=button],[role=option]){cursor:pointer}:where(:disabled,label:has(>:disabled,+disabled)){cursor:not-allowed}:where(a){color:inherit;text-underline-offset:.2ex}:where(ul,ol){list-style:none}:where(img,svg,video,canvas,audio,iframe,embed,object){display:block}:where(img,picture,svg,video){block-size:auto;max-inline-size:100%}:where(p,h1,h2,h3,h4,h5,h6){overflow-wrap:break-word}:where(h1,h2,h3){text-wrap:balance;line-height:calc(1em + .5rem)}:where(hr){block-size:0;border:none;border-block-start:1px solid;color:inherit;overflow:visible}:where(dialog,[popover]){background:none;border:none;color:inherit;inset:unset;max-height:unset;max-width:unset;overflow:unset}:where(dialog:not([open],[popover]),[popover]:not(:popover-open)){display:none!important}:where(:focus-visible){box-shadow:0 0 0 5px Canvas;outline:3px solid CanvasText;outline-offset:1px}:where(:focus-visible,:target){scroll-margin-block:8vh}:where(.visually-hidden:not(:focus-within,:active)){border:0!important;clip-path:inset(50%)!important;height:1px!important;overflow:hidden!important;position:absolute!important;-webkit-user-select:none!important;user-select:none!important;white-space:nowrap!important;width:1px!important}body{background-color:#fff}.rpjxp40{align-items:center;color:var(--uxe4kj1);display:flex;flex-direction:column;font-family:-apple-system,Roboto,sans-serif,serif;justify-content:start;margin:0;min-height:100svh;padding:0;width:100svw}.rpjxp41{height:100%;padding:16px;width:100%}.rpjxp42{background-color:var(--uxe4kj3);display:flex;justify-content:center;width:100%}.rpjxp43{flex-grow:1}.rpjxp44{font-style:italic;margin:16px 0;text-align:center}@media screen and (min-width:860px){.rpjxp41{width:860px}}.rnrds60{color:inherit;text-decoration:inherit}.rnrds61{color:var(--uxe4kj1)}.rnrds61:hover{color:var(--uxe4kj2)}.kfauuc0{display:flex;flex-direction:row;justify-content:space-between;max-width:860px}.kfauuc1{flex-shrink:1}.kfauuc2{font-size:32px;font-weight:400}.kfauuc3{font-size:16px;font-weight:300}.kfauuc4{align-items:center;display:flex;flex-shrink:1;justify-content:center;padding:8px}.kfauuc5{font-size:14px;font-weight:700;text-decoration-line:none}@media screen and (min-width:860px){.kfauuc5{font-size:18px}}._1b32skb0{border-radius:8px;margin:46px 0 0;transition:background-color .2s ease-in-out,transform .1s ease-in-out}._1b32skb0:hover{background-color:rgba(0,0,0,.05);transform:translateY(-2px)}._1b32skb1{color:inherit;display:block;padding:16px 2px;text-decoration:none}._1b32skb2{font-size:24px;font-weight:400}._1b32skb3{font-size:14px;font-weight:400}._1b32skb4{display:flex;flex-direction:row;gap:8px;justify-content:center;margin-top:24px}@media screen and (min-width:860px){._1b32skb1{padding:16px}}</style><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){const t=e.target;if(void 0===t.dataset.mainImage)return;if(void 0===t.dataset.gatsbyImageSsr)return;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><link rel="sitemap" type="application/xml" href="/sitemap-index.xml"/><link rel="icon" href="/favicon-32x32.png?v=9dc3f101a4981124067c5705dab8bfbc" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=9dc3f101a4981124067c5705dab8bfbc"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=9dc3f101a4981124067c5705dab8bfbc"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=9dc3f101a4981124067c5705dab8bfbc"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=9dc3f101a4981124067c5705dab8bfbc"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=9dc3f101a4981124067c5705dab8bfbc"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=9dc3f101a4981124067c5705dab8bfbc"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=9dc3f101a4981124067c5705dab8bfbc"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=9dc3f101a4981124067c5705dab8bfbc"/><link rel="alternate" type="application/rss+xml" title="RSS Feed of blog.nasustim.com" href="/rss.xml"/><title data-gatsby-head="true">Ollama 導入と VSCode との連携 | NASUSTIM</title><link rel="canonical" href="https://blog.nasustim.com/entry/introduce-ollama-into-macos" data-gatsby-head="true"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="uxe4kj0 rpjxp40"><div class="rpjxp42"><div class="rpjxp41"><header class="kfauuc0"><div class="kfauuc1"><a class="rnrds60" href="https://blog.nasustim.com/"><h1 class="kfauuc2">NASUSTIM</h1><h2 class="kfauuc3">Mitsuhiro HIBINO</h2></a></div><div class="kfauuc4"><a class="rnrds61" href="https://nasustim.com/" target="_blank" rel="noreferrer"><span class="kfauuc5">[ABOUT ME]</span></a></div></header></div></div><div class="rpjxp41 rpjxp43"><main><div class="_2danq60"><div class="_2danq61"><small>June 07, 2025</small><h2 class="_2danq62">Ollama 導入と VSCode との連携</h2></div><article class="_2danq61 _2danq63"><p>IT 業界では常に話題の中心にいる LLM。2025年の現在、僕も職場では LLM ツール<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>にどっぷり浸かる毎日を過ごしています。しかし、高性能なモデルや API 経由での利用をしたい場合は追加課金が発生することが多いです。</p>
<p>ローカルマシンで LLM をホストできれば課金の必要はありません。また、導入プロセスを通じて LLM への理解をより深めたいと考えました。</p>
<p>そこでこの記事では、ローカルマシンへの LLM の導入から VSCode 上でコード編集機能を利用するまでにやったことを記録として残します。</p>
<p>動作を確認した環境は以下の通りです。</p>
<ul>
<li>PC
<ul>
<li>Macbook Air M2 (2022年)</li>
<li>RAM 16GB</li>
</ul>
</li>
<li>OS
<ul>
<li>macOS Sequoia v15.3 (24D60)</li>
</ul>
</li>
</ul>
<h2>調査</h2>
<p>Gemini に頼りました。</p>
<p><img src="/introduce-ollama-into-macos_01.png" alt="Gemini の回答"/></p>
<p>会話リンク: <a href="https://g.co/gemini/share/779a99352554">https://g.co/gemini/share/779a99352554</a></p>
<p>指示された手順はざっくり以下の通りです。</p>
<ol>
<li><a href="http://ollama.com/">Ollama</a> をインストールして LLM を起動
<ul>
<li>HTTP で LLM とやり取りできるサーバがバンドルされているらしい</li>
</ul>
</li>
<li>VSCode に <a href="https://www.continue.dev/">Continue</a> の拡張をインストールして ①の LLM を指定する</li>
<li>LLM を使ってコードを書ける！</li>
</ol>
<h2>導入の流れ</h2>
<h3>1. Ollama 導入</h3>
<p>似たソフトウェアとして <a href="https://lmstudio.ai/">LM Studio</a> や、複数の LLM を一つのホストに集約することに主眼を置いたルーター系（勝手に命名しました）のプロジェクト<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>がありますが、Ollama はそれらと比較したときに以下のようなアドバンテージがあると理解しました。</p>
<ul>
<li>高いポータビリティ
<ul>
<li>コマンドラインのみで導入可能</li>
<li>システムプロンプト等のカスタマイズ内容も Modelfile という Dockerfile ライクな形式のファイルで表現可能<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup></li>
</ul>
</li>
<li>VSCode への導入サポートの手厚さ
<ul>
<li>前述の Continue を使って楽チン</li>
</ul>
</li>
</ul>
<p>今回は Gemini の指示通り Ollama で進めます。</p>
<pre><code class="language-bash"># ollama を Homebrew でインストール
$ brew install ollama

# ollama のサーバを起動
# この後は起動したまま別のターミナルから操作するか、コマンド末尾に `&amp;` をつけてバックグラウンドジョブとして動かすかはお好みで
$ ollama serve
</code></pre>
<p>ollama では様々なモデルを利用できます。（<a href="https://ollama.com/search">モデル一覧</a>）
最初は軽量かつ突飛な動作をしないであろうモデルを試 したいと考え、Google が開発した Gemma 3 の 1B パラメータモデル<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup>を選択しました。</p>
<pre><code class="language-bash">$ ollama pull gemma3:1b
</code></pre>
<p>コマンドライン経由でのチャットも行えるようです。</p>
<p><img src="/introduce-ollama-into-macos_02.png" alt="Gemma 3 の回答"/></p>
<p>シャ、シャベッタ！</p>
<p>ローカル LLM 導入のファーストステップはクリアです ✅</p>
<h3>2. VSCode に Continue をインストール &amp; 設定</h3>

















<table><thead><tr><th>説明</th><th>スクリーンショット</th></tr></thead><tbody><tr><td>インストール後初期画面</td><td><img src="/introduce-ollama-into-macos_03.png" alt="Continue"/></td></tr><tr><td>Get started を開くとこの画面が開くので Ollama を選択する</td><td><img src="/introduce-ollama-into-macos_04.png" alt="Continue"/></td></tr></tbody></table>
<h3>3. Ollama(Gemma 3)にコードを編集させる</h3>
<p>手順2の設定を終えたら、モデル一覧から Gemma 3 が選択できるようになっています。</p>
<p><img src="/introduce-ollama-into-macos_05.png" alt="Continue"/></p>
<p>チャットは問題なさそうです。</p>
<p><img src="/introduce-ollama-into-macos_06.png" alt="Continue"/></p>
<p>コードを選択して Add Highlighted Code to Context でチャット画面に指示のコンテクストとして追加できます。Copilot / Cursor と同じインターフェースですね。</p>
<p><img src="/introduce-ollama-into-macos_07.png" alt="Continue"/></p>
<p>うーん、これじゃない...業務で使う Claude 3.7 Sonnet 辺りのモデルよりも明らかな知識/文脈理解の不足を感じます。より賢くてコーディングに特化したモデルを使う必要がありそうです。</p>
<p>ということでコーディング特化モデルの中で　pull 数が  多くサイズも大きすぎない <code>qwen2.5-coder:7b</code> を試しました</p>
<p><a href="https://ollama.com/library/qwen2.5-coder">https://ollama.com/library/qwen2.5-coder</a></p>
<p><img src="/introduce-ollama-into-macos_08.png" alt="Qwen 2.5 coder"/></p>
<p>こちらは期待通りの出力が得られました 🙌</p>
<h2>まとめ</h2>
<p>シンプルな手順で LLM 導入 + VSCode 連携まで構築できました。</p>
<p>また、シンプルなタスクであれば精度高く実行できるモデルがローカルでホストできることも確認できました。</p>
<p>個人用途ならローカルで十分かもしれません。夢が広がりますね</p>
<hr/>
<section data-footnotes="" class="footnotes"><h2 class="sr-only" id="footnote-label">Footnotes</h2>
<ol>
<li id="user-content-fn-1">
<p>記事執筆時点では、調べ物は ChatGPT（プライベートでは Gemini と Grok を使い分け）、一つの文書を元にしたい場合は NotebookLM、コーディング時は GitHub Copilot と Devin をよく利用しています <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>サクッと調べると <a href="https://github.com/kcolemangt/llm-router">LLM Router</a>, <a href="https://github.com/lm-sys/RouteLLM">RouteLLM</a> 辺りの情報が多いように感じました <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-3">
<p><a href="https://ollama.readthedocs.io/en/modelfile/">https://ollama.readthedocs.io/en/modelfile/</a> <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3" class="data-footnote-backref">↩</a></p>
</li>
<li id="user-content-fn-4">
<p><a href="https://ollama.com/library/gemma3">https://ollama.com/library/gemma3</a> ; 1b=10億パラメータの機械学習モデルをラップトッ  プでサクッと動かせることにビビりました <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section></article></div></main></div><footer class="rpjxp44"><p>© nasustim, 2010-</p></footer></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-3YY246MS11"></script><script>
      
      
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-3YY246MS11', {"send_page_view":false});
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/entry/introduce-ollama-into-macos/";/*]]>*/</script><!-- slice-start id="_gatsby-scripts-1" -->
          <script
            id="gatsby-chunk-mapping"
          >
            window.___chunkMapping="{\"app\":[\"/app-c07d364c41761e17168b.js\"],\"component---src-page-components-entry-article-tsx\":[\"/component---src-page-components-entry-article-tsx-272cc278a001a7bb05d2.js\"],\"component---src-page-components-index-tsx\":[\"/component---src-page-components-index-tsx-bbf7dacf1d751803998f.js\"]}";
          </script>
        <script>window.___webpackCompilationHash="f5516a3e6b69ae313a1b";</script><script src="/webpack-runtime-75b0118a29c35e9e207a.js" async></script><script src="/framework-ab160c1ebcd2dab818c1.js" async></script><script src="/app-c07d364c41761e17168b.js" async></script><!-- slice-end id="_gatsby-scripts-1" --></body></html>